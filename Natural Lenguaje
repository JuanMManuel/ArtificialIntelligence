# -*- coding: utf-8 -*-
"""TrabajoFinalIA1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u1dw1yoWNbtpdnY8ehxIh25nhPrirUk-
"""

#Cargamos librerías principales

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import nltk

nltk.download('popular')

import tweepy as tw
import re

consumer_key= 'iRfOZZCWjfI6QGX0bJFUM73vO'

consumer_secret= 'DhcYDy0xu79iui0cOEirWva8QXVRWlRpwhZHEJcJB9XgGXcrUr'

access_token= '403050612-Oz3qrjcLwiy968EToTR2KmdvjUlhhsZmsxQdv9wl'

access_token_secret= 'qX5QYegME3WnnsGIh80ZVDfgDp55tckxsYPIxOJVKdP7x'

#Conexión
auth = tw.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
api = tw.API(auth, wait_on_rate_limit=True)

#Configura la consulta

search_words = "elecciones presidenciales Colombia"
#Realiza la consulta
tweets = tw.Cursor(api.search,
              q=search_words,
              lang="es").items(500)
#Crea un dataframe con los resultados              
resultados = [[tweet.user.screen_name, tweet.user.location, tweet.text, tweet.retweet_count, tweet.source, tweet.created_at ] for tweet in tweets]
data = pd.DataFrame(data=resultados, columns=['user', "location","text", "retweet_count", "source", "date"])
data.to_excel("datos_twitter.xlsx")
data

#Concatenamos todos los textos

all_tweets=' '.join(data['text'])

all_tweets

#Cargamos los datos

data = pd.read_excel("datos_twitter.xlsx", sheet_name=0)

data.head()

#Convertir a minúscula, limpiar y tokenizar

from nltk.tokenize import word_tokenize

def tokenizar(texto):
  tokens = word_tokenize(texto)
  words = [w.lower() for w in tokens if w.isalnum()]
  return words

data['tokens'] = data['text'].apply(lambda x: tokenizar(x))
data.head()

#Eliminamos stopwords y otras palabras

from nltk.corpus import stopwords

sw= stopwords.words('spanish')
sw.append("https") #adiciona nuevas palabras a la lista de stopwords
sw.append("rt")
sw.append("RT")



def limpiar_stopwords(lista):

  clean_tokens = lista[:]
  for token in lista:
    

    if token in sw:
      clean_tokens.remove(token)
  return clean_tokens


# Limpiamos los tokens

data['sin_stopwords'] = data['tokens'].apply(lambda x: limpiar_stopwords(x))

data.head()

#Reducción a la raíz (Stemming)

from nltk.stem import SnowballStemmer

stemmer = SnowballStemmer('spanish')

def stem_tokens(lista):

  lista_stem = []
  for token in lista:
    lista_stem.append(stemmer.stem(token))
  return lista_stem

data['stemming'] = data['sin_stopwords'].apply(lambda x: stem_tokens(x))
data.head()

#Nube de palabras 

from wordcloud import WordCloud


lista_palabras = data["stemming"].tolist()

tokens = [keyword.strip() for sublista in lista_palabras for keyword in sublista]
texto= ' '.join(tokens)
wc = WordCloud(background_color="white", max_words=1000, margin=0)
wc.generate(texto)
wc.to_file("nube1.png")

plt.figure(figsize=(15,15))
plt.imshow(wc, interpolation='bilinear')
plt.axis("off")

plt.show()

#Gráfica de palabras mas frecuentes de stemming

freq = nltk.FreqDist(tokens)

plt.figure(figsize=(8, 8))

freq.plot(20, cumulative=False)

#Representación en vector de características tf*idf

from sklearn.feature_extraction.text import TfidfVectorizer

def dummy_fun(doc):
    return doc

tfidf = TfidfVectorizer(

    analyzer='word',
    tokenizer=dummy_fun,
    preprocessor=dummy_fun,
    token_pattern=None)  


X = tfidf.fit_transform(data['stemming']) #stemming, lemmatization
data_tfidf=pd.DataFrame(X.todense(),columns=tfidf.get_feature_names())
data_tfidf

#Método del codo para encontrar la mejor cantidad de clusters 
from sklearn.cluster import KMeans 

ks = range(2, 20)  # crear valores del 2 al 10
inertias = []

for k in ks:
    # Crear  modelo
    model = KMeans(n_clusters=k)
    model.fit(data_tfidf)
    inertias.append(model.inertia_)
    
# Graficar cantidad de clusters vs inertias
plt.plot(ks, inertias, '-o')
plt.xlabel('Numero de clusters, k')
plt.ylabel('inertia')
plt.xticks(ks)

from sklearn.cluster import KMeans 
model = KMeans(n_clusters=8, max_iter=100)
model.fit(data_tfidf)

#Evaluacion del Modelo
print(model.inertia_)

#Centroides de los clusters
centroides=pd.DataFrame(model.cluster_centers_, columns=tfidf.get_feature_names())
centroides.round(0)

#Cluster asignado a cada registro
data['cluster']=model.predict(data_tfidf)
data

for cluster in data['cluster'].unique():
    data_cluster = data[data['cluster'] == cluster ].reset_index()
    lista_palabras = data_cluster["stemming"].tolist()
    tokens_abstractos = [keyword.strip() for sublista in lista_palabras for keyword in sublista]
    wordcloud = WordCloud( max_words=1000,margin=0).generate((" ").join(tokens_abstractos))
    plt.figure()
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    plt.show()
