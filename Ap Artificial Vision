# -*- coding: utf-8 -*-
"""TrabajoVisionArtificial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q3N1uEzn0xTXqiXODw9UsOcIGbibv3M6
"""

#Cargamos librerías principales

import numpy as np

import pandas as pd

import matplotlib.pyplot as plt



# Tratamiento de imágenes

from PIL import Image



# Lectura del directorio de imágenes

import os, sys

# Lectura de la carpeta con las imágenes de casas 

path = "casas"

files = os.listdir( path )




mat_hist=[] # Matriz con los histogramas

nombres=[] #Nombres de los archivos



# Se leen las imágenes y se calcula el histograma

for f in files:

  if f.endswith('.jpg'):

    #Nombre de la imagen

    print(f)

    nombres.append(f)

    #Leemos la imagen

    img = Image.open(path+'/'+ f)

    img=img.resize((300,300))

    # Calculamos histograma de color

    hist = img.histogram()

    #Adicionamos el histograma a la matriz 

    mat_hist.append(hist)

    

# Data frame con los histogramas

casas=pd.DataFrame(mat_hist)

casas['clase']="casa"

# Lectura de la carpeta con las imágenes de apartamentos 

path = "edificios"

files = os.listdir( path )




mat_hist=[] # Matriz con los histogramas

nombres=[] #Nombres de los archivos



# Se leen las imágenes y se calcula el histograma

for f in files:

  if f.endswith('.jpg'):

    #Nombre de la imagen

    print(f)

    nombres.append(f)

    #Leemos la imagen

    img = Image.open(path+'/'+ f)

    img=img.resize((300,300))

    # Calculamos histograma de color

    hist = img.histogram()

    #Adicionamos el histograma a la matriz 

    mat_hist.append(hist)

    

# Data frame con los histogramas

edificios=pd.DataFrame(mat_hist)

edificios['clase']="edificios"

#Unimos los dos conjuntos de datos

data = pd.concat([casas, edificios], axis=0)

data

#Se codifican las categorias de la variable Objetivo
from sklearn.preprocessing import LabelEncoder
labelencoder= LabelEncoder()
data["clase"]=labelencoder.fit_transform(data["clase"])
data.head()

#Division 70-30

from sklearn.model_selection import train_test_split
X= data.drop("clase", axis=1) #Variables predictoras
Y= data['clase'] #Variable Objetivo
X_train, X_test, Y_train, Y_test= train_test_split(X,Y, test_size=0.3, stratify=Y)
Y_train.value_counts().plot(kind='bar')

#Deep
#Arquitectura de la red neuronal profunda
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
model_deep = Sequential()

model_deep.add(Dense(400, input_dim=768, activation='sigmoid')) #35 entradas y capa oculta 30 neuronas

model_deep.add(Dense(250, activation='sigmoid')) #capa oculta de 10 neuronas

model_deep.add(Dense(2, activation='softmax')) #capa de salida (valores de la variable objetivo)



#Aprendizaje

optimizer = keras.optimizers.SGD(learning_rate=0.03,momentum=0.5)

model_deep.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) 

history=model_deep.fit(X_train, Y_train, epochs=500)

plt.plot(history.history['loss'])

loss, acc = model_deep.evaluate(X_test, Y_test, verbose=0) #30%
print(loss)
print(acc)

#Guardar el modelo
model_deep.save('model_deep.h5')

#Guardamos el labelencoder y los nombres de columnas si son necesarios

import pickle

filename = 'labelencoder.pkl'

pickle.dump(labelencoder, open(filename, 'wb'))
